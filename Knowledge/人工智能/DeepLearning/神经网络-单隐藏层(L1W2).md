# 神经网络-单隐藏层
## 神经网络的工作方式
### 计算的整体流程
对于每一个神经元, 整体流程如下
z =w^T^ x + b  ->  a = sigmoid(z)
其中x为上一层的输入, w为此神经元的权重
a为本神经元的激活值

## 正向传播
对于一个只有一个隐藏层的神经网络
定义激活函数为
sigmoid(z) = g(z)
其导数写为g'(z)

第一步:
`z[1] = W[1]X + b[1]   # W[1]表示第一层的权重矩阵, X表示输入样本`
第二步:
`A[1] = g[1](Z[1]) # A[1]表示第一层的激活值`
第三步:
`Z[2] = W[2]A[1] + b[2]`
第四步:
`A[2] = g[2](Z[2]) `
A[2]即为神经网络的输出, 正向传播计算完毕

## 反向传播
目标: 寻找合适的W[1], b[1], W[2], b[2], 使得成本函数`J(W[1], b[1], W[2], b[2]) = (1/m) * sum( L(y^, y) )`最小

一步梯度下降的步骤
1. 利用正向传播计算预测值y^
2. 计算dW[1], db[1], dW[2], db[2]
3. 更新W[1], W[2], b[1], b[2]
W[1] = W[1] - a * dW[1] # a表示学习率(learing rate)
问题的关键就是如何计算dW[1], db[1], dW[2], db[2]

### 计算dW[1], db[1], dW[2], db[2]
在Logistic回归中, 已知:
dw = dz * dx
db = dz

而在神经网络中, 根据链式法则
`dz = dL / dz = (dL/dz) * (da / dz) = da * g'(z)  # a = g(z), 故da / dz = g'(z)`
故`dz[2] = a[2] - y`
`dW[2] = dL(a[2], y) / dW[2] = (dL(a[2], y) / dz[2] ) * (d=z[2] / dW[2]) = dz[2] * a[1]`
同理
`db[2] = dZ[2]`

`dz[1] = dL / dz[1] = (dL / dz[2]) * (dz[2] / dz[1]) = dz[2] * W[2] * g[1]'z[1]`
故可以计算出
dW[1] = dz[1]x^T^
db[1] = dz[1]

### 反向传播总结

dz[2] = a[2] - y
dW[2] = dz[2]a[1]^T^
db[2] = dz[2]
dz[1] = W[2]^T^dz[2] * g[1]'z[1]
dW[1] = dz[1]x^T^
db[1] = dz[1]

## 代码框架
```


```




